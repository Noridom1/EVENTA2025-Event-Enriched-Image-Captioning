{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fdeb93",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b59b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af214994",
   "metadata": {},
   "source": [
    "### Spliting CNN and Guardian urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27848a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls from CNN: 24200\n",
      "Number of urls from Guardian: 178603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('database.json', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Remove control characters (ASCII < 32 except for newline and tab)\n",
    "content = re.sub(r'(?<!\\\\)[\\x00-\\x1F]', ' ', content)\n",
    "database = json.loads(content)\n",
    "\n",
    "cnn_articles = [article_id for article_id in database.keys() if 'https://www.cnn.com' in database[article_id]['url']]\n",
    "guardian_articles = [article_id for article_id in database.keys() if 'https://www.theguardian.com' in database[article_id]['url']]\n",
    "\n",
    "print(f\"Number of urls from CNN: {len(cnn_articles)}\")\n",
    "print(f\"Number of urls from Guardian: {len(guardian_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422f72",
   "metadata": {},
   "source": [
    "### Crawling CNN's captions\n",
    "\n",
    "- The captions of the images are crawled, if the image has no caption, its alt-text is crawled instead.\n",
    "- For the alt-texts, there will be a token `<alt>` at the end.\n",
    "\n",
    "### Instruction:\n",
    "- Adjust the start_index and end_index \n",
    "- start_index = last end_index\n",
    "- Recommend: `end_index - start_index <= 2000`\n",
    "- The file will be saved with format: `database_{start_index}_{end_index}.json` in `database_with_captions`\n",
    "- If there is error during running, rename manually the file `database_modified.json` to the right name (adjust the end_index to fit with the progress log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc4fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [To Modify] the start index and end index of the urls\n",
    "start_index = 4000\n",
    "end_index = 4200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01da3f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "[Progress INFO] Processed 4050/200 (2025.00% done)\n",
      "Saving results...\n",
      "[Progress INFO] Processed 4100/200 (2050.00% done)\n",
      "Saving results...\n",
      "[Progress INFO] Processed 4150/200 (2075.00% done)\n",
      "Saving results...\n",
      "[Progress INFO] Processed 4200/200 (2100.00% done)\n",
      "Finished scraping to index 4200 with 0 failed requests!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx, end_idx, user_agent=\"MyNewsBot\", progress_step=50):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids[start_idx:end_idx])\n",
    "    cnt = 0\n",
    "    # Retrieve and read the robots.txt file for each website\n",
    "    for article_id in article_ids[start_idx:end_idx]:\n",
    "\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\", class_=\"article__main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "                    for image_block in main_content.find_all('div', class_='image'):\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        if image_block.find_parent(class_= 'video-resource__image'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('div', class_='image__caption')\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if cnt % progress_step == 0:\n",
    "            save_database(database_modified)\n",
    "            print('[Progress INFO] Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.2, 0.6))\n",
    "\n",
    "    print(f\"Finished scraping to index {end_index} with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, fileroot='database_with_captions', filename='database_modified.json'):\n",
    "    filename = os.path.join(fileroot, filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "def rename_file(filepath, new_filepath):\n",
    "    os.rename(filepath, new_filepath)\n",
    "\n",
    "# Save the modified database after chunks of progress_step urls.\n",
    "send_requests_to_urls(cnn_articles, start_idx=start_index, end_idx=end_index, progress_step=50)\n",
    "\n",
    "# Rename database file with start_index and end_index\n",
    "db_path = os.path.join('database_with_captions', 'database_modified.json')\n",
    "new_dp_path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "rename_file(db_path, new_dp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6537495",
   "metadata": {},
   "source": [
    "### Crawling Guardian's captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx=0, user_agent=\"MyNewsBot\"):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids)\n",
    "    cnt = start_idx\n",
    "\n",
    "    for article_id in article_ids[start_idx:]:\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        # print(f\"robots_url: {robots_url}\")\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        # Check if the URL is allowed by robots.txt for the given user-agent\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\", class_=\"article__main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "                    for image_block in main_content.find_all('div', class_='image'):\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        if image_block.find_parent(class_= 'video-resource__image'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('div', class_='image__caption')\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "            # Handle 429 Too Many Requests (rate-limiting)\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            # Handle other errors like 503 (Service Unavailable), 404, etc.\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if failed_requests > 0:\n",
    "            time.sleep(random.uniform(2 ** failed_requests, 10))\n",
    "\n",
    "        if cnt % 50 == 0:\n",
    "            print(\"Saving results...\")\n",
    "            save_database(database_modified)\n",
    "            print('Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "\n",
    "    print(f\"Finished scraping with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, filename='database_modified.json'):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "article_ids = []\n",
    "with open('ArticleIDs/article_ids1.json', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "content = re.sub(r'(?<!\\\\)[\\x00-\\x1F]', ' ', content)\n",
    "article_ids = json.loads(content)\n",
    "\n",
    "send_requests_to_urls(cnn_articles, start_idx=1050)\n",
    "save_database(database=database_modified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
