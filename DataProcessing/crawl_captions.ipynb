{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fdeb93",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b59b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af214994",
   "metadata": {},
   "source": [
    "### Spliting CNN and Guardian urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27848a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls from CNN: 24200\n",
      "Number of urls from Guardian: 178603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../data/database/database.json', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Remove control characters (ASCII < 32 except for newline and tab)\n",
    "content = re.sub(r'(?<!\\\\)[\\x00-\\x1F]', ' ', content)\n",
    "database = json.loads(content)\n",
    "\n",
    "cnn_articles = [article_id for article_id in database.keys() if 'https://www.cnn.com' in database[article_id]['url']]\n",
    "guardian_articles = [article_id for article_id in database.keys() if 'https://www.theguardian.com' in database[article_id]['url']]\n",
    "\n",
    "print(f\"Number of urls from CNN: {len(cnn_articles)}\")\n",
    "print(f\"Number of urls from Guardian: {len(guardian_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422f72",
   "metadata": {},
   "source": [
    "### Crawling CNN's captions\n",
    "\n",
    "- The captions of the images are crawled, if the image has no caption, its alt-text is crawled instead.\n",
    "- For the alt-texts, there will be a token `<alt>` at the end.\n",
    "\n",
    "### Instruction:\n",
    "- Adjust the start_index and end_index \n",
    "- start_index = last end_index\n",
    "- Recommend: `end_index - start_index <= 2000`\n",
    "- The file will be saved with format: `database_{start_index}_{end_index}.json` in `database_with_captions`\n",
    "- If there is error during running, rename manually the file `database_modified.json` to the right name (adjust the end_index to fit with the progress log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [To Modify] the start index and end index of the urls\n",
    "start_index = 13000\n",
    "end_index = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01da3f86",
   "metadata": {
    "tags": [
     "Run",
     "CNN scraping"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawling captions with from index 13000 to index 15000\n",
      "[Progress INFO] Processed 13050/24200 (53.93% done)\n",
      "[Progress INFO] Processed 13100/24200 (54.13% done)\n",
      "[Progress INFO] Processed 13150/24200 (54.34% done)\n",
      "[Progress INFO] Processed 13200/24200 (54.55% done)\n",
      "[Progress INFO] Processed 13250/24200 (54.75% done)\n",
      "[Progress INFO] Processed 13300/24200 (54.96% done)\n",
      "[Progress INFO] Processed 13350/24200 (55.17% done)\n",
      "[Progress INFO] Processed 13400/24200 (55.37% done)\n",
      "[Progress INFO] Processed 13450/24200 (55.58% done)\n",
      "[Progress INFO] Processed 13500/24200 (55.79% done)\n",
      "[Progress INFO] Processed 13550/24200 (55.99% done)\n",
      "[Progress INFO] Processed 13600/24200 (56.20% done)\n",
      "[Progress INFO] Processed 13650/24200 (56.40% done)\n",
      "[Progress INFO] Processed 13700/24200 (56.61% done)\n",
      "[Progress INFO] Processed 13750/24200 (56.82% done)\n",
      "[Progress INFO] Processed 13800/24200 (57.02% done)\n",
      "[Progress INFO] Processed 13850/24200 (57.23% done)\n",
      "[Progress INFO] Processed 13900/24200 (57.44% done)\n",
      "[Progress INFO] Processed 13950/24200 (57.64% done)\n",
      "[Progress INFO] Processed 14000/24200 (57.85% done)\n",
      "[Progress INFO] Processed 14050/24200 (58.06% done)\n",
      "[Progress INFO] Processed 14100/24200 (58.26% done)\n",
      "[Progress INFO] Processed 14150/24200 (58.47% done)\n",
      "[Progress INFO] Processed 14200/24200 (58.68% done)\n",
      "[Progress INFO] Processed 14250/24200 (58.88% done)\n",
      "[Progress INFO] Processed 14300/24200 (59.09% done)\n",
      "[Progress INFO] Processed 14350/24200 (59.30% done)\n",
      "[Progress INFO] Processed 14400/24200 (59.50% done)\n",
      "[Progress INFO] Processed 14450/24200 (59.71% done)\n",
      "[Progress INFO] Processed 14500/24200 (59.92% done)\n",
      "[Progress INFO] Processed 14550/24200 (60.12% done)\n",
      "[Progress INFO] Processed 14600/24200 (60.33% done)\n",
      "[Progress INFO] Processed 14650/24200 (60.54% done)\n",
      "[Progress INFO] Processed 14700/24200 (60.74% done)\n",
      "[Progress INFO] Processed 14750/24200 (60.95% done)\n",
      "[Progress INFO] Processed 14800/24200 (61.16% done)\n",
      "[Progress INFO] Processed 14850/24200 (61.36% done)\n",
      "[Progress INFO] Processed 14900/24200 (61.57% done)\n",
      "[Progress INFO] Processed 14950/24200 (61.78% done)\n",
      "[Progress INFO] Processed 15000/24200 (61.98% done)\n",
      "Finished scraping to index 15000 with 0 failed requests!\n"
     ]
    }
   ],
   "source": [
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx, end_idx, user_agent=\"MyNewsBot\", progress_step=50):\n",
    "    print(f\"Starting crawling captions with from index {start_idx} to index {end_idx}\")\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids)\n",
    "    cnt = start_idx\n",
    "    # Retrieve and read the robots.txt file for each website\n",
    "    for article_id in article_ids[start_idx:end_idx]:\n",
    "\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\", class_=\"article__main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "                    for image_block in main_content.find_all('div', class_='image'):\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        if image_block.find_parent(class_= 'video-resource__image'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('div', class_='image__caption')\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if cnt % progress_step == 0:\n",
    "            save_database(database_modified)\n",
    "            print('[Progress INFO] Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "    print(f\"Finished scraping to index {end_index} with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, fileroot='database_with_captions', filename='database_modified.json'):\n",
    "    filename = os.path.join(fileroot, filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "def rename_file(filepath, new_filepath):\n",
    "    os.rename(filepath, new_filepath)\n",
    "\n",
    "# Save the modified database after chunks of progress_step urls.\n",
    "send_requests_to_urls(cnn_articles, start_idx=start_index, end_idx=end_index, progress_step=50)\n",
    "\n",
    "# Rename database file with start_index and end_index\n",
    "db_path = os.path.join('database_with_captions', 'database_modified.json')\n",
    "new_dp_path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "rename_file(db_path, new_dp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448e1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database path: database_with_captions\\database_13000_15000.json\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ac014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions', 'database_modified.json')\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dcd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "database_folder = 'database_with_captions'\n",
    "database_cnn_folder = 'database_cnn'\n",
    "database_cnn_dir = os.path.join(database_cnn_folder, 'database_cnn.json')\n",
    "\n",
    "database_cnn = {}\n",
    "\n",
    "for db_name in os.listdir(database_folder):\n",
    "    db_path = os.path.join(database_folder, db_name)\n",
    "    with open(db_path, 'r') as f:\n",
    "        db = json.load(f)\n",
    "        database_cnn.update(db)\n",
    "\n",
    "print(len(database_cnn))\n",
    "with open(database_cnn_dir, 'w') as f:\n",
    "    json.dump(database_cnn, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6537495",
   "metadata": {},
   "source": [
    "### Crawling Guardian's captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2540ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "end_index = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawling captions with from index 0 to index 50\n",
      "[Progress INFO] Processed 1/178603 (0.00% done)\n",
      "[Progress INFO] Processed 2/178603 (0.00% done)\n",
      "[Progress INFO] Processed 3/178603 (0.00% done)\n",
      "[Progress INFO] Processed 4/178603 (0.00% done)\n",
      "[Progress INFO] Processed 5/178603 (0.00% done)\n",
      "[Progress INFO] Processed 6/178603 (0.00% done)\n",
      "[Progress INFO] Processed 7/178603 (0.00% done)\n",
      "[Progress INFO] Processed 8/178603 (0.00% done)\n",
      "[Progress INFO] Processed 9/178603 (0.01% done)\n",
      "[Progress INFO] Processed 10/178603 (0.01% done)\n",
      "[Progress INFO] Processed 11/178603 (0.01% done)\n",
      "[Progress INFO] Processed 12/178603 (0.01% done)\n",
      "[Progress INFO] Processed 13/178603 (0.01% done)\n",
      "[Progress INFO] Processed 14/178603 (0.01% done)\n",
      "[Progress INFO] Processed 15/178603 (0.01% done)\n",
      "[Progress INFO] Processed 16/178603 (0.01% done)\n",
      "[Progress INFO] Processed 17/178603 (0.01% done)\n",
      "[Progress INFO] Processed 18/178603 (0.01% done)\n",
      "[Progress INFO] Processed 19/178603 (0.01% done)\n",
      "[Progress INFO] Processed 20/178603 (0.01% done)\n",
      "[Progress INFO] Processed 21/178603 (0.01% done)\n",
      "[Progress INFO] Processed 22/178603 (0.01% done)\n",
      "[Progress INFO] Processed 23/178603 (0.01% done)\n",
      "[Progress INFO] Processed 24/178603 (0.01% done)\n",
      "[Progress INFO] Processed 25/178603 (0.01% done)\n",
      "[Progress INFO] Processed 26/178603 (0.01% done)\n",
      "[Progress INFO] Processed 27/178603 (0.02% done)\n",
      "[Progress INFO] Processed 28/178603 (0.02% done)\n",
      "[Progress INFO] Processed 29/178603 (0.02% done)\n",
      "[Progress INFO] Processed 30/178603 (0.02% done)\n",
      "[Progress INFO] Processed 31/178603 (0.02% done)\n",
      "[Progress INFO] Processed 32/178603 (0.02% done)\n",
      "[Progress INFO] Processed 33/178603 (0.02% done)\n",
      "[Progress INFO] Processed 34/178603 (0.02% done)\n",
      "[Progress INFO] Processed 35/178603 (0.02% done)\n",
      "[Progress INFO] Processed 36/178603 (0.02% done)\n",
      "[Progress INFO] Processed 37/178603 (0.02% done)\n",
      "[Progress INFO] Processed 38/178603 (0.02% done)\n",
      "[Progress INFO] Processed 39/178603 (0.02% done)\n",
      "[Progress INFO] Processed 40/178603 (0.02% done)\n",
      "[Progress INFO] Processed 41/178603 (0.02% done)\n",
      "[Progress INFO] Processed 42/178603 (0.02% done)\n",
      "[Progress INFO] Processed 43/178603 (0.02% done)\n",
      "[Progress INFO] Processed 44/178603 (0.02% done)\n",
      "[Progress INFO] Processed 45/178603 (0.03% done)\n",
      "[Progress INFO] Processed 46/178603 (0.03% done)\n",
      "[Progress INFO] Processed 47/178603 (0.03% done)\n",
      "[Progress INFO] Processed 48/178603 (0.03% done)\n",
      "[Progress INFO] Processed 49/178603 (0.03% done)\n",
      "[Progress INFO] Processed 50/178603 (0.03% done)\n",
      "Finished scraping to index 50 with 0 failed requests!\n"
     ]
    }
   ],
   "source": [
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx, end_idx, user_agent=\"MyNewsBot\", progress_step=50):\n",
    "    print(f\"Starting crawling captions with from index {start_idx} to index {end_idx}\")\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids)\n",
    "    cnt = start_idx\n",
    "    # Retrieve and read the robots.txt file for each website\n",
    "    for article_id in article_ids[start_idx:end_idx]:\n",
    "\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "\n",
    "                    image_blocks = main_content.find_all('div', class_= 'dcr-1t8m8f2')\n",
    "\n",
    "                    if not image_blocks:\n",
    "                        image_blocks = main_content.find_all('div', class_ = 'dcr-hlfdy3')\n",
    "\n",
    "                    for image_block in image_blocks:\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        if image_block.find_parent(class_= 'video-resource__image'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('span', class_='dcr-1qvd3m6')\n",
    "\n",
    "                        # print(caption_tag)\n",
    "                        if not caption_tag:\n",
    "                            parent = image_block.find_parent()\n",
    "                            # print(parent)\n",
    "                            caption_tag = parent.find('span', class_='dcr-1qvd3m6')\n",
    "\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if cnt % progress_step == 0:\n",
    "            save_database(database_modified)\n",
    "            print('[Progress INFO] Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.2))\n",
    "\n",
    "    print(f\"Finished scraping to index {end_index} with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, fileroot='database_with_captions_guardian', filename='database_modified.json'):\n",
    "    filename = os.path.join(fileroot, filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "def rename_file(filepath, new_filepath):\n",
    "    os.rename(filepath, new_filepath)\n",
    "\n",
    "# Save the modified database after chunks of progress_step urls.\n",
    "send_requests_to_urls(guardian_articles, start_idx=start_index, end_idx=end_index, progress_step=50)\n",
    "\n",
    "# Rename database file with start_index and end_index\n",
    "db_path = os.path.join('database_with_captions_guardian', 'database_modified.json')\n",
    "new_dp_path = os.path.join('database_with_captions_guardian', 'database_') + f'{start_index}_{end_index}.json'\n",
    "rename_file(db_path, new_dp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
