{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fdeb93",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b59b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af214994",
   "metadata": {},
   "source": [
    "### Spliting CNN and Guardian urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27848a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls from CNN: 24200\n",
      "Number of urls from Guardian: 178603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../data/database/database.json', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Remove control characters (ASCII < 32 except for newline and tab)\n",
    "content = re.sub(r'(?<!\\\\)[\\x00-\\x1F]', ' ', content)\n",
    "database = json.loads(content)\n",
    "\n",
    "cnn_articles = [article_id for article_id in database.keys() if 'https://www.cnn.com' in database[article_id]['url']]\n",
    "guardian_articles = [article_id for article_id in database.keys() if 'https://www.theguardian.com' in database[article_id]['url']]\n",
    "\n",
    "print(f\"Number of urls from CNN: {len(cnn_articles)}\")\n",
    "print(f\"Number of urls from Guardian: {len(guardian_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422f72",
   "metadata": {},
   "source": [
    "### Crawling CNN's captions\n",
    "\n",
    "- The captions of the images are crawled, if the image has no caption, its alt-text is crawled instead.\n",
    "- For the alt-texts, there will be a token `<alt>` at the end.\n",
    "\n",
    "### Instruction:\n",
    "- Adjust the start_index and end_index \n",
    "- start_index = last end_index\n",
    "- Recommend: `end_index - start_index <= 2000`\n",
    "- The file will be saved with format: `database_{start_index}_{end_index}.json` in `database_with_captions`\n",
    "- If there is error during running, rename manually the file `database_modified.json` to the right name (adjust the end_index to fit with the progress log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [To Modify] the start index and end index of the urls\n",
    "start_index = 13000\n",
    "end_index = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01da3f86",
   "metadata": {
    "tags": [
     "Run",
     "CNN scraping"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawling captions with from index 13000 to index 15000\n",
      "[Progress INFO] Processed 13050/24200 (53.93% done)\n",
      "[Progress INFO] Processed 13100/24200 (54.13% done)\n",
      "[Progress INFO] Processed 13150/24200 (54.34% done)\n",
      "[Progress INFO] Processed 13200/24200 (54.55% done)\n",
      "[Progress INFO] Processed 13250/24200 (54.75% done)\n",
      "[Progress INFO] Processed 13300/24200 (54.96% done)\n",
      "[Progress INFO] Processed 13350/24200 (55.17% done)\n",
      "[Progress INFO] Processed 13400/24200 (55.37% done)\n",
      "[Progress INFO] Processed 13450/24200 (55.58% done)\n",
      "[Progress INFO] Processed 13500/24200 (55.79% done)\n",
      "[Progress INFO] Processed 13550/24200 (55.99% done)\n",
      "[Progress INFO] Processed 13600/24200 (56.20% done)\n",
      "[Progress INFO] Processed 13650/24200 (56.40% done)\n",
      "[Progress INFO] Processed 13700/24200 (56.61% done)\n",
      "[Progress INFO] Processed 13750/24200 (56.82% done)\n",
      "[Progress INFO] Processed 13800/24200 (57.02% done)\n",
      "[Progress INFO] Processed 13850/24200 (57.23% done)\n",
      "[Progress INFO] Processed 13900/24200 (57.44% done)\n",
      "[Progress INFO] Processed 13950/24200 (57.64% done)\n",
      "[Progress INFO] Processed 14000/24200 (57.85% done)\n",
      "[Progress INFO] Processed 14050/24200 (58.06% done)\n",
      "[Progress INFO] Processed 14100/24200 (58.26% done)\n",
      "[Progress INFO] Processed 14150/24200 (58.47% done)\n",
      "[Progress INFO] Processed 14200/24200 (58.68% done)\n",
      "[Progress INFO] Processed 14250/24200 (58.88% done)\n",
      "[Progress INFO] Processed 14300/24200 (59.09% done)\n",
      "[Progress INFO] Processed 14350/24200 (59.30% done)\n",
      "[Progress INFO] Processed 14400/24200 (59.50% done)\n",
      "[Progress INFO] Processed 14450/24200 (59.71% done)\n",
      "[Progress INFO] Processed 14500/24200 (59.92% done)\n",
      "[Progress INFO] Processed 14550/24200 (60.12% done)\n",
      "[Progress INFO] Processed 14600/24200 (60.33% done)\n",
      "[Progress INFO] Processed 14650/24200 (60.54% done)\n",
      "[Progress INFO] Processed 14700/24200 (60.74% done)\n",
      "[Progress INFO] Processed 14750/24200 (60.95% done)\n",
      "[Progress INFO] Processed 14800/24200 (61.16% done)\n",
      "[Progress INFO] Processed 14850/24200 (61.36% done)\n",
      "[Progress INFO] Processed 14900/24200 (61.57% done)\n",
      "[Progress INFO] Processed 14950/24200 (61.78% done)\n",
      "[Progress INFO] Processed 15000/24200 (61.98% done)\n",
      "Finished scraping to index 15000 with 0 failed requests!\n"
     ]
    }
   ],
   "source": [
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx, end_idx, user_agent=\"MyNewsBot\", progress_step=50):\n",
    "    print(f\"Starting crawling captions with from index {start_idx} to index {end_idx}\")\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids)\n",
    "    cnt = start_idx\n",
    "    # Retrieve and read the robots.txt file for each website\n",
    "    for article_id in article_ids[start_idx:end_idx]:\n",
    "\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\", class_=\"article__main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "                    for image_block in main_content.find_all('div', class_='image'):\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        if image_block.find_parent(class_= 'video-resource__image'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('div', class_='image__caption')\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if cnt % progress_step == 0:\n",
    "            save_database(database_modified)\n",
    "            print('[Progress INFO] Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "    print(f\"Finished scraping to index {end_index} with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, fileroot='database_with_captions', filename='database_modified.json'):\n",
    "    filename = os.path.join(fileroot, filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "def rename_file(filepath, new_filepath):\n",
    "    os.rename(filepath, new_filepath)\n",
    "\n",
    "# Save the modified database after chunks of progress_step urls.\n",
    "send_requests_to_urls(cnn_articles, start_idx=start_index, end_idx=end_index, progress_step=50)\n",
    "\n",
    "# Rename database file with start_index and end_index\n",
    "db_path = os.path.join('database_with_captions', 'database_modified.json')\n",
    "new_dp_path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "rename_file(db_path, new_dp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448e1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database path: database_with_captions\\database_13000_15000.json\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions', 'database_') + f'{start_index}_{end_index}.json'\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ac014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions', 'database_modified.json')\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dcd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "database_folder = 'database_with_captions'\n",
    "database_cnn_folder = 'database_cnn'\n",
    "database_cnn_dir = os.path.join(database_cnn_folder, 'database_cnn.json')\n",
    "\n",
    "database_cnn = {}\n",
    "\n",
    "for db_name in os.listdir(database_folder):\n",
    "    db_path = os.path.join(database_folder, db_name)\n",
    "    with open(db_path, 'r') as f:\n",
    "        db = json.load(f)\n",
    "        database_cnn.update(db)\n",
    "\n",
    "print(len(database_cnn))\n",
    "with open(database_cnn_dir, 'w') as f:\n",
    "    json.dump(database_cnn, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6537495",
   "metadata": {},
   "source": [
    "### Crawling Guardian's captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2540ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 5500\n",
    "end_index = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0775a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawling captions with from index 5500 to index 8000\n",
      "[Progress INFO] Processed 5550/178603 (3.11% done)\n",
      "[Progress INFO] Processed 5600/178603 (3.14% done)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m     os.rename(filepath, new_filepath)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Save the modified database after chunks of progress_step urls.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43msend_requests_to_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguardian_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Rename database file with start_index and end_index\u001b[39;00m\n\u001b[32m    101\u001b[39m db_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33mdatabase_with_captions_guardian\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdatabase_modified.json\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36msend_requests_to_urls\u001b[39m\u001b[34m(article_ids, start_idx, end_idx, user_agent, progress_step)\u001b[39m\n\u001b[32m     82\u001b[39m         save_database(database_modified)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[Progress INFO] Processed \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[33m done)\u001b[39m\u001b[33m'\u001b[39m % (cnt, N, cnt*\u001b[32m100.0\u001b[39m/N))\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished scraping to index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailed_requests\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed requests!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "database_modified = defaultdict(dict)\n",
    "\n",
    "def send_requests_to_urls(article_ids, start_idx, end_idx, user_agent=\"MyNewsBot\", progress_step=50):\n",
    "    print(f\"Starting crawling captions with from index {start_idx} to index {end_idx}\")\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    failed_requests = 0\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "    N = len(article_ids)\n",
    "    cnt = start_idx\n",
    "    # Retrieve and read the robots.txt file for each website\n",
    "    for article_id in article_ids[start_idx:end_idx]:\n",
    "\n",
    "        database_modified[article_id] = database[article_id].copy()\n",
    "\n",
    "        url = database[article_id]['url']\n",
    "        robots_url = f\"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt\"\n",
    "        robot_parser.set_url(robots_url)\n",
    "        robot_parser.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not robot_parser.can_fetch(user_agent, url):\n",
    "            print(f\"❌ BLOCKED by robots.txt - {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                main_content = soup.find(\"main\")\n",
    "                captions = []\n",
    "\n",
    "                if main_content:\n",
    "\n",
    "                    image_blocks = main_content.find_all('div', class_= ['dcr-1t8m8f2', 'dcr-xzr4d9', 'dcr-hlfdy3'])\n",
    "\n",
    "                    # if not image_blocks:\n",
    "                    #     image_blocks = main_content.find_all('div', class_ = 'dcr-hlfdy3')\n",
    "\n",
    "                    for image_block in image_blocks:\n",
    "                        if image_block.find_parent('a'):\n",
    "                            continue\n",
    "\n",
    "                        caption_tag = image_block.find('span', class_='dcr-1qvd3m6')\n",
    "\n",
    "                        # print(caption_tag)\n",
    "                        if not caption_tag:\n",
    "                            parent = image_block.find_parent()\n",
    "                            # print(parent)\n",
    "                            caption_tag = parent.find('span', class_='dcr-1qvd3m6')\n",
    "\n",
    "                        caption = caption_tag.get_text(separator=' ', strip=True) if caption_tag else \"\"\n",
    "\n",
    "                        if caption == \"\":\n",
    "                            img = image_block.find('img')\n",
    "                            if img:\n",
    "                                alt_text = img.get('alt', '').strip()\n",
    "                                if alt_text:\n",
    "                                    caption = alt_text + ' <alt>'\n",
    "                        captions.append(caption)\n",
    "\n",
    "                seen = set()\n",
    "                captions = [caption for caption in captions if not (caption in seen or seen.add(caption))]\n",
    "                database_modified[article_id]['captions'] = captions\n",
    "\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"❌ TOO MANY REQUESTS - {url}. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                failed_requests += 1\n",
    "\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"❌ ERROR - Status Code: {response.status_code} - {url}\")\n",
    "                failed_requests += 1\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ REQUEST FAILED - {url} due to {e}\")\n",
    "            failed_requests += 1\n",
    "\n",
    "        if cnt % progress_step == 0:\n",
    "            save_database(database_modified)\n",
    "            print('[Progress INFO] Processed %d/%d (%.2f%% done)' % (cnt, N, cnt*100.0/N))\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.2))\n",
    "\n",
    "    print(f\"Finished scraping to index {end_index} with {failed_requests} failed requests!\")\n",
    "\n",
    "def save_database(database, fileroot='database_with_captions_guardian', filename='database_modified.json'):\n",
    "    filename = os.path.join(fileroot, filename)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(database, f, indent=4)\n",
    "\n",
    "def rename_file(filepath, new_filepath):\n",
    "    os.rename(filepath, new_filepath)\n",
    "\n",
    "# Save the modified database after chunks of progress_step urls.\n",
    "send_requests_to_urls(guardian_articles, start_idx=start_index, end_idx=end_index, progress_step=50)\n",
    "\n",
    "# Rename database file with start_index and end_index\n",
    "db_path = os.path.join('database_with_captions_guardian', 'database_modified.json')\n",
    "new_dp_path = os.path.join('database_with_captions_guardian', 'database_') + f'{start_index}_{end_index}.json'\n",
    "rename_file(db_path, new_dp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aba4fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database path: database_with_captions_guardian\\database_modified.json\n",
      "1150\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the result\n",
    "\n",
    "path = os.path.join('database_with_captions_guardian', 'database_modified.json')\n",
    "with open(path, 'r') as f:\n",
    "    print(f'database path: {path}')\n",
    "    tmp_database = json.load(f)\n",
    "    print(len(tmp_database))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
